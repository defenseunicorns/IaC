data "aws_eks_cluster_auth" "this" {
  name = module.eks_blueprints.eks_cluster_id
}

data "aws_availability_zones" "available" {
  filter {
    name   = "opt-in-status"
    values = ["opt-in-not-required"]
  }
}

data "aws_partition" "current" {}

#---------------------------------------------------------------
# EKS Blueprints
#---------------------------------------------------------------

module "eks_blueprints" {
  # pending approval of [PR](https://github.com/aws-ia/terraform-aws-eks-blueprints/issues/1387)
  source = "git::https://github.com/ntwkninja/terraform-aws-eks-blueprints.git?ref=v4.21.1"

  cluster_name    = local.cluster_name
  cluster_version = var.eks_k8s_version

  vpc_id             = var.vpc_id
  private_subnet_ids = var.private_subnet_ids
  # public_subnet_ids  = var.public_subnet_ids
  cluster_endpoint_public_access  = var.cluster_endpoint_public_access
  cluster_endpoint_private_access = var.cluster_endpoint_private_access
  control_plane_subnet_ids        = var.control_plane_subnet_ids

  #----------------------------------------------------------------------------------------------------------#
  # Security groups used in this module created by the upstream modules terraform-aws-eks (https://github.com/terraform-aws-modules/terraform-aws-eks).
  #   Upstream module implemented Security groups based on the best practices doc https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html.
  #   So, by default the security groups are restrictive. Users needs to enable rules for specific ports required for App requirement or Add-ons
  #   See the notes below for each rule used in these examples
  #----------------------------------------------------------------------------------------------------------#
  cluster_security_group_additional_rules = {
    ingress_bastion_to_cluster = {
      # name        = "allow bastion ingress to cluster"
      description              = "Bastion SG to Cluster"
      security_group_id        = module.eks_blueprints.cluster_security_group_id
      from_port                = 443
      to_port                  = 443
      protocol                 = "tcp"
      type                     = "ingress"
      source_security_group_id = var.source_security_group_id
    }
  }

  node_security_group_additional_rules = {
    # Extend node-to-node security group rules. Recommended and required for the Add-ons
    ingress_self_all = {
      description = "Node to node all ports/protocols"
      protocol    = "-1"
      from_port   = 0
      to_port     = 0
      type        = "ingress"
      self        = true
    }
    # Recommended outbound traffic for Node groups
    egress_all = {
      description      = "Node all egress"
      protocol         = "-1"
      from_port        = 0
      to_port          = 0
      type             = "egress"
      cidr_blocks      = ["0.0.0.0/0"]
      ipv6_cidr_blocks = ["::/0"]
    }
    # Allows Control Plane Nodes to talk to Worker nodes on all ports. Added this to simplify the example and further avoid issues with Add-ons communication with Control plane.
    # This can be restricted further to specific port based on the requirement for each Add-on e.g., metrics-server 4443, spark-operator 8080, karpenter 8443 etc.
    # Change this according to your security requirements if needed
    ingress_cluster_to_node_all_traffic = {
      description                   = "Cluster API to Nodegroup all traffic"
      protocol                      = "-1"
      from_port                     = 0
      to_port                       = 0
      type                          = "ingress"
      source_cluster_security_group = true
    }
  }

  cluster_kms_key_additional_admin_arns = var.cluster_kms_key_additional_admin_arns

  map_users = var.aws_auth_eks_map_users
  map_roles = [
    {
      rolearn  = aws_iam_role.auth_eks_role.arn
      username = aws_iam_role.auth_eks_role.name
      groups   = ["system:masters"]
    },
    {
      rolearn  = var.bastion_role_arn
      username = var.bastion_role_name
      groups   = ["system:masters"]
    }
  ]
  #---------------------------------------------------------------
  # EKS Blueprints - Self Managed Node Groups
  #---------------------------------------------------------------

  self_managed_node_groups = { 
    self_mg1 = {

      node_group_name        = var.node_group_name
      launch_template_os     = var.launch_template_os
      subnet_ids             = var.private_subnet_ids
      create_launch_template = var.create_launch_template
      launch_template_os     = var.launch_template_os # amazonlinux2eks or bottlerocket or windows
      custom_ami_id          = var.custom_ami_id      # Bring your own custom AMI generated by Packer/ImageBuilder/Puppet etc.

      create_iam_role           = var.create_iam_role                           # Changing `create_iam_role=false` to bring your own IAM Role
      iam_role_arn              = aws_iam_role.self_managed_ng.arn              # custom IAM role for aws-auth mapping; used when create_iam_role = false
      iam_instance_profile_name = aws_iam_instance_profile.self_managed_ng.name # IAM instance profile name for Launch templates; used when create_iam_role = false

      format_mount_nvme_disk = var.format_mount_nvme_disk
      public_ip              = var.public_ip
      enable_monitoring      = var.enable_monitoring

      placement = {
        affinity          = null
        availability_zone = null
        group_name        = null
        host_id           = null
        tenancy           = var.tenancy
      }

      enable_metadata_options = var.enable_metadata_options

      pre_userdata = var.pre_userdata

      # bootstrap_extra_args used only when you pass custom_ami_id. Allows you to change the Container Runtime for Nodes
      # e.g., bootstrap_extra_args="--use-max-pods false --container-runtime containerd"
      bootstrap_extra_args = var.bootstrap_extra_args

      block_device_mappings = var.block_device_mappings

      instance_type = var.instance_type
      desired_size  = var.desired_size
      max_size      = var.max_size
      min_size      = var.min_size
      capacity_type = "" # Optional Use this only for SPOT capacity as  capacity_type = "spot"

      k8s_labels = {
        Environment = "preprod"
        Zone        = "test"
      }

      additional_tags = {
        ExtraTag    = "m5x-on-demand"
        Name        = "m5x-on-demand"
        subnet_type = "private"
      }
    }
  }

  #   }
  #   self_mg5 = {
  #     node_group_name = var."${node_group_name}5" # Name is used to create a dedicated IAM role for each node group and adds to AWS-AUTH config map

  #     subnet_type            = "private"
  #     subnet_ids             = var.private_subnet_ids # Optional defaults to Private Subnet Ids used by EKS Control Plane
  #     create_launch_template = true
  #     launch_template_os     = "amazonlinux2eks" # amazonlinux2eks or bottlerocket or windows
  #     custom_ami_id          = ""                # Bring your own custom AMI generated by Packer/ImageBuilder/Puppet etc.

  #     create_iam_role           = false                                         # Changing `create_iam_role=false` to bring your own IAM Role
  #     iam_role_arn              = aws_iam_role.self_managed_ng.arn              # custom IAM role for aws-auth mapping; used when create_iam_role = false
  #     iam_instance_profile_name = aws_iam_instance_profile.self_managed_ng.name # IAM instance profile name for Launch templates; used when create_iam_role = false

  #     format_mount_nvme_disk = true
  #     public_ip              = false
  #     enable_monitoring      = false

  #     enable_metadata_options = false

  #     pre_userdata = <<-EOT
  #       yum install -y amazon-ssm-agent
  #       systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent
  #     EOT

  #     post_userdata = "" # Optional config

  #     # --node-labels is used to apply Kubernetes Labels to Nodes
  #     # --register-with-taints used to apply taints to Nodes
  #     # e.g., kubelet_extra_args='--node-labels=WorkerType=SPOT,noderole=spark --register-with-taints=spot=true:NoSchedule --max-pods=58',
  #     kubelet_extra_args = "--node-labels=WorkerType=SPOT,noderole=spark --register-with-taints=test=true:NoSchedule --max-pods=20"

  #     # bootstrap_extra_args used only when you pass custom_ami_id. Allows you to change the Container Runtime for Nodes
  #     # e.g., bootstrap_extra_args="--use-max-pods false --container-runtime containerd"
  #     bootstrap_extra_args = "--use-max-pods false"

  #     block_device_mappings = [
  #       {
  #         device_name = "/dev/xvda" # mount point to /
  #         volume_type = "gp3"
  #         volume_size = 50
  #       },
  #       {
  #         device_name = "/dev/xvdf" # mount point to /local1 (it could be local2, depending upon the disks are attached during boot)
  #         volume_type = "gp3"
  #         volume_size = 80
  #         iops        = 3000
  #         throughput  = 125
  #       },
  #       {
  #         device_name = "/dev/xvdg" # mount point to /local2 (it could be local1, depending upon the disks are attached during boot)
  #         volume_type = "gp3"
  #         volume_size = 100
  #         iops        = 3000
  #         throughput  = 125
  #       }
  #     ]

  #     instance_type = "m5.large"
  #     desired_size  = 2
  #     max_size      = 10
  #     min_size      = 2
  #     capacity_type = "" # Optional Use this only for SPOT capacity as  capacity_type = "spot"

  #     k8s_labels = {
  #       Environment = "preprod"
  #       Zone        = "test"
  #       WorkerType  = "SELF_MANAGED_ON_DEMAND"
  #     }

  #     additional_tags = {
  #       ExtraTag    = "m5x-on-demand"
  #       Name        = "m5x-on-demand"
  #       subnet_type = "private"
  #     }
  #   }

  #   spot_2vcpu_8mem = {
  #     node_group_name    = "smng-spot-2vcpu-8mem"
  #     capacity_type      = "spot"
  #     capacity_rebalance = true
  #     instance_types     = ["m5.large", "m4.large", "m6a.large", "m5a.large", "m5d.large"]
  #     min_size           = 0
  #     subnet_ids         = var.private_subnet_ids
  #     launch_template_os = "amazonlinux2eks" # amazonlinux2eks or bottlerocket
  #     k8s_taints         = [{ key = "spotInstance", value = "true", effect = "NO_SCHEDULE" }]
  #   }

  #   spot_4vcpu_16mem = {
  #     node_group_name    = "smng-spot-4vcpu-16mem"
  #     capacity_type      = "spot"
  #     capacity_rebalance = true
  #     instance_types     = ["m5.xlarge", "m4.xlarge", "m6a.xlarge", "m5a.xlarge", "m5d.xlarge"]
  #     min_size           = 0
  #     subnet_ids         = var.private_subnet_ids
  #     launch_template_os = "amazonlinux2eks" # amazonlinux2eks or bottlerocket
  #     k8s_taints         = [{ key = "spotInstance", value = "true", effect = "NO_SCHEDULE" }]
  #   }
  # }
}

#---------------------------------------------------------------
# Custom IAM role for Self Managed Node Group
#---------------------------------------------------------------

data "aws_iam_policy_document" "self_managed_ng_assume_role_policy" {
  statement {
    sid = "EKSWorkerAssumeRole"

    actions = [
      "sts:AssumeRole",
    ]
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
  }
}

resource "aws_iam_role" "self_managed_ng" {
  name                  = "${var.name}-self-managed-node-role"
  description           = "EKS Managed Node group IAM Role"
  assume_role_policy    = data.aws_iam_policy_document.self_managed_ng_assume_role_policy.json
  path                  = "/"
  force_detach_policies = true
  managed_policy_arns = [
    "arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy",
    "arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKS_CNI_Policy",
    "arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
    "arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonSSMManagedInstanceCore"
  ]

  tags = local.tags
}

resource "aws_iam_instance_profile" "self_managed_ng" {
  name = "${var.name}-self-managed-node-instance-profile"
  role = aws_iam_role.self_managed_ng.name
  path = "/"

  lifecycle {
    create_before_destroy = true
  }

  tags = local.tags
}

resource "aws_iam_role" "auth_eks_role" {
  name               = "${var.name}-auth-eks-role"
  description        = "EKS AuthConfig Role"
  assume_role_policy = <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Action": "sts:AssumeRole",
            "Principal": {
               "AWS": ${var.cluster_kms_key_additional_admin_arns == [] ? "[]" : jsonencode(var.cluster_kms_key_additional_admin_arns)}
            },
            "Effect": "Allow",
            "Sid": ""
        }
    ]
}
EOF
}
